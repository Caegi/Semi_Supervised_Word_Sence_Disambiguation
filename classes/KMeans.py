# -*- coding: utf-8 -*-
"""KMEANS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSbW6GU1xNZ7WlvhWi5h2Eg_YY_vvj63

# Applying KMEANS on Iris Dataset :

---
"""

from sklearn.datasets import load_iris
import numpy as np
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
import random
from sklearn.metrics import silhouette_score
import numpy as np

# Function to initialize centroids randomly
def init_centroids(k, iris_array):
    random_indices = np.random.choice(range(len(iris_array)), size=k, replace=False)
    centroids = iris_array[random_indices]
    return centroids

# Function to calculate cosine similarity between centroids and all examples
def get_new_clusters(iris_array,centroids):
  clusters = []
  for vector in iris_array
    similarities = []
    for i in range(len(centroids)):

      dot_product = np.dot(vector,centroids[i])
      m_vector = np.linalg.norm(vector)
      m_centroid = np.linalg.norm(centroids[i])
      similarity = dot_product / (m_vector * m_centroid)
      similarities.append(similarity)

    max_value = max(similarities)
    max_index = similarities.index(max_value)
    clusters.append(max_index)

  return clusters



def calculate_silhouette_coefficient(df):
    silhouette_avg = silhouette_score(df.iloc[:, :-1], df['cluster'])
    return silhouette_avg


# Function to calculate new centroids
def get_new_centroids(df, cs):
    new_centroids = []
    for c in cs:
      centroid = np.mean(df[df['cluster'] == c].iloc[:, :-1], axis=0)

      new_centroids.append(centroid)
    return (np.array(new_centroids))[:,:4]




# Main function
def k_means(df, k):

    iris_array = df.iloc[:, :4].values  # Excluding last column which is cluster label
    centroids = init_centroids(k, iris_array)

    is_changed = True
    i = 0
    while is_changed:
        print(i)
        i+=1

        clusters_before = df['cluster'].copy()  # Making a copy of current clusters for comparison late

        new_clusters = get_new_clusters(iris_array,centroids)

        df['cluster'] = new_clusters

        if not np.array_equal(np.array(new_clusters), clusters_before):

            class_unique = df['cluster'].unique()
            centroids =  get_new_centroids(df, class_unique)

        else:
            is_changed = False
        silhouette_coefficient = calculate_silhouette_coefficient(df)

    return df, centroids


# Function to visualize clusters in 3D
def visualize_clusters_3d(df, centroids):
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    for cluster in df['cluster'].unique():
        cluster_points = df[df['cluster'] == cluster].iloc[:, :-1].values
        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], cluster_points[:, 2], label=f'Cluster {cluster}', alpha=0.7)

    ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], color='black', marker='*', label='Centroids', s=200)
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')
    ax.set_zlabel('Feature 3')
    ax.set_title('Clustering of Data Points in 3D')
    ax.legend()
    ax.grid(True)

    plt.show()

if __name__ == "__main__":


    iris = load_iris()

    df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])

    k = 3

    df['cluster'] = 0

    df, centroids = k_means(df, k)

    print(df['cluster'].value_counts())

# Visualtization clusters
visualize_clusters_3d(df, centroids)

class kmeans:

  def __init__(self,df:pd.DataFrame,k:int):

    self.df = df
    self.k = k
    self.centroids = None

    print("classe created successfuly")

  # Methode to initialize centroids randomly
  def _init_centroids(self,iris_array):
    print('initalizing centroids en cours')
    random_indices = np.random.choice(range(len(iris_array)), size=self.k, replace=False)
    centroids = iris_array[random_indices]
    return centroids

  #  to calculate new centroids
  def _get_new_centroids(self,df, cs):
      new_centroids = []
      for c in cs:
        centroid = np.mean(df[df['cluster'] == c].iloc[:, :-1], axis=0)

        new_centroids.append(centroid)
      return (np.array(new_centroids))[:,:4]

  # Methode  to calculate cosine similarity between centroids and all examples:
  def _get_new_clusters(self,iris_array,centroids):

    clusters = []
    for vector in iris_array:
      similarities = []
      for i in range(len(centroids)):

        dot_product = np.dot(vector,centroids[i])
        m_vector = np.linalg.norm(vector)
        m_centroid = np.linalg.norm(centroids[i])
        similarity = dot_product / (m_vector * m_centroid)
        similarities.append(similarity)

      max_value = max(similarities) ;  max_index = similarities.index(max_value)
      clusters.append(max_index)

    return clusters



  def fit(self):

    self.df['cluster'] = 0

     # Excluding last column which is cluster label
    iris_array = self.df.iloc[:, :4].values
    self.centroids = self._init_centroids(iris_array)
    #print(self.centroids)

    is_changed = True

    while is_changed:

      # Making a copy of current clusters for comparison late
      clusters_before = self.df['cluster'].copy()
      new_clusters = self._get_new_clusters(iris_array,self.centroids)

      #print(new_clusters)
      self.df['cluster'] = new_clusters

      if not np.array_equal(np.array(new_clusters), clusters_before):

            class_unique = df['cluster'].unique()
            self.centroids =  self._get_new_centroids(self.df, class_unique)

      else:
            is_changed = False


  def get_centroids(self):
      return self.centroids


  def get_dataframe(self):
      return self.df


  def get_distribution(self):
    return self.df['cluster'].value_counts()


    # Function to visualize clusters in 3D
  def visualize(self):
      fig = plt.figure(figsize=(10, 8))
      ax = fig.add_subplot(111, projection='3d')

      for cluster in df['cluster'].unique():
          cluster_points = self.df[self.df['cluster'] == cluster].iloc[:, :-1].values
          ax.scatter(cluster_points[:, 0], cluster_points[:, 1], cluster_points[:, 2], label=f'Cluster {cluster}', alpha=0.7)

      ax.scatter(self.centroids[:, 0], self.centroids[:, 1], self.centroids[:, 2], color='black', marker='*', label='Centroids', s=200)
      ax.set_xlabel('Feature 1')
      ax.set_ylabel('Feature 2')
      ax.set_zlabel('Feature 3')
      ax.set_title('Clustering of Data Points in 3D')
      ax.legend()
      ax.grid(True)

      plt.show()

# Loading Data
iris = load_iris()

# Preprocessing
df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])

# Number of clusters
k = 3

# Instanciate Kmeans Clustering
my_kmeans = kmeans(df,k)
my_kmeans.fit()



cendroids = my_kmeans.get_centroids()
df = my_kmeans.get_dataframe()

print(my_kmeans.get_distribution())


my_kmeans.visualize()

"""# JeanThomas code prepocessing Data:"""

import xml.etree.ElementTree as ET # to extract data from XML file
import pandas as pd
import re
!wget http://www.llf.cnrs.fr/dataset/fse/FSE-1.1-10_12_19.tar.gz
!tar -xzvf "/content/FSE-1.1-10_12_19.tar.gz" -C "/content/"     #[run this cell to extract tar.gz files]

def get_df_from_xml(filepath_sent, filepath_ws):
    """
    Get a pandas dataframe containing sentences, their target words (words with multiple meanings),
     their lemma, their position in the sentence, and their word sence in the current context

    Parameters:
       filepath_sent: file with the xml file containing the sentences
        and the targets words to disambiguate
       filepath_ws: text file containing the gold clases, i.e. the words with
        multiples meaning and what word sence is used in the sentence

    Returns:
        pandas dataframe
    """
    tree = ET.parse(filepath_sent)
    root = tree.getroot()  # root = corpus
    l_sent = []
    l_target = []
    l_pos = []

    for sentence_block in root.iter("sentence"):
        s, targets = get_sentence_and_targets(sentence_block)
        update_lists(l_pos, l_sent, l_target, s, targets)

    l_word_sence, l_lemma = get_word_sence_and_lemma(filepath_ws)

    return pd.DataFrame({"sentence": l_sent, "target": l_target, "lemma": l_lemma,
                         "position": l_pos, "word_sence": l_word_sence})

def get_sentence_and_targets(sentence_block):
    """
    get the sentence and the target words from a sentence xml "block"

    Parameters:
      sentence_block: "block" of xml containing a tag(its name),
      a text(what it contains), and children

    Returns:
      s: str
      targets: list[str]
    """
    s = []
    targets = [] # this is a list to account for sentences with multiple targets
    for child in sentence_block:
        s.append(child.text)

        if (child.tag == "instance"):
            targets.append(child.text)

    s = " ".join(s)
    return s, targets


def update_lists(l_pos, l_sent, l_target, s, targets):
    """
    add values to l_pos, l_sent, l_target with the values s and targets

    Parameters:
      l_pos: list[ tuple(int, int) ], list of the indices of the target words in the sentences they are in
      l_sent: list[str], list of sentences
      l_target: list[str] list of target words (words to disambiguate)
      s: str sentence
      targets: list[str]

    Returns:
      None
    """
    for target in targets:
        l_sent.append(s)

        l_target.append(target)

        target_start = s.index(target)
        target_end = target_start + len(target) - 1
        l_pos.append( (target_start, target_end) )

def get_word_sence_and_lemma(filepath_ws):
  """

  Parameters:
    filepath_ws: textfile containing the target words appearing in the xml file
     in order and their gold labels (word sence in the context of the sentence)

  Returns:
    l_word_sence: list[str]Â´
    l_lemma: list[str]
  """
  with open(filepath_ws) as file:
    content = file.read()

    pattern_word_sence = re.compile("ws_[0-9]_.*?(?=_)|ws_[0-9]\w_.*?(?=_)")
    l_word_sence = re.findall(pattern_word_sence, content, flags=0)

    pattern_lemma = re.compile("(?<=ws_[0-9]_).*?(?=_)|(?<=ws_[0-9]\w_).*?(?=_)")
    l_lemma = re.findall(pattern_lemma, content, flags=0)

    return l_word_sence, l_lemma

#get_df_from_xml("/content/FSE-1.1.data.xml", "/content/FSE-1.1.gold.key.txt")
df = get_df_from_xml("/content/FSE-1.1-191210/FSE-1.1.data.xml", "/content/FSE-1.1-191210/FSE-1.1.gold.key.txt")

df_aboutir = df[df['lemma']=="aboutir"]

!pip install fasttext
import fasttext
!pip install spacy
!python -m spacy download en_core_web_md
import spacy

# Load the English model with word vectors
nlp = spacy.load("en_core_web_md")

df_aboutir['sentence'] = df_aboutir['sentence'].apply(lambda x : nlp(x).vector)

np.vstack(df_aboutir['sentence'].values).shape

embeddings_array = np.vstack(df_aboutir['sentence'].values)

embeddings_array.shape

